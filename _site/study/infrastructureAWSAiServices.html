<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>AWS AI Services ¬∑ glucolte</title>
  <meta name="description" content="notes ‚Ä¢ principles ‚Ä¢ systems">
  <link rel="stylesheet" href="/assets/style.css">
  <link rel="stylesheet" href="/assets/study.css">
  <link rel="icon" type="image/svg+xml" href="/favicon.svg">
  <link rel="shortcut icon" href="/favicon.ico">
  <link rel="apple-touch-icon" href="/favicon.png">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>AWS AI Services | glucolte</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="AWS AI Services" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="notes ‚Ä¢ principles ‚Ä¢ systems" />
<meta property="og:description" content="notes ‚Ä¢ principles ‚Ä¢ systems" />
<link rel="canonical" href="http://localhost:4000/study/infrastructureAWSAiServices" />
<meta property="og:url" content="http://localhost:4000/study/infrastructureAWSAiServices" />
<meta property="og:site_name" content="glucolte" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="AWS AI Services" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"notes ‚Ä¢ principles ‚Ä¢ systems","headline":"AWS AI Services","url":"http://localhost:4000/study/infrastructureAWSAiServices"}</script>
<!-- End Jekyll SEO tag -->

</head>
<body>
  <header class="site-header">
  <a class="brand" href="/">glucolte</a>
  <nav class="nav">
    <a href="/principles">principles</a>
    <a href="/rules">rules</a>
    <a href="/stretches">stretches</a>
    <a href="/study/">study</a>
    <a href="/projects/">projects</a>
    <a href="/setup/">setup</a>
  </nav>
  <button id="dark-mode-toggle" class="dark-mode-toggle" aria-label="Toggle dark mode">
    <svg class="sun-icon" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
    <svg class="moon-icon" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
    </svg>
  </button>
</header>


  <main class="container">
    <article class="content">
      <h2 id="section-1-foundation-vs-llm">1. Foundation Models vs LLMs</h2>

<p>Foundation models are massive neural networks trained on diverse data (text, code, audio, video, images) so they can be adapted to many downstream tasks. Large Language Models (LLMs) are a subset focused on token-based language tasks. Use the matrix below for quick exam recall.</p>

<table class="study-table">
  <thead>
    <tr>
      <th>Dimension</th>
      <th>Foundation Model</th>
      <th>Large Language Model (LLM)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Scope</strong></td>
      <td>Multi-modal (text, audio, image, video, code) or unimodal</td>
      <td>Primarily language + code; some add image adapters</td>
    </tr>
    <tr>
      <td><strong>Training Task</strong></td>
      <td>Generic self-supervised objectives (masking, contrastive learning, etc.)</td>
      <td>Autoregressive next-token prediction</td>
    </tr>
    <tr>
      <td><strong>Input / Output</strong></td>
      <td>Any supported modality, embeddings, or metadata</td>
      <td>Tokens (text/code) with optional tool-calls</td>
    </tr>
    <tr>
      <td><strong>Adaptation</strong></td>
      <td>Task-specific fine-tuning, adapters, RLHF</td>
      <td>Prompting, instruction tuning, RLHF, guardrails</td>
    </tr>
    <tr>
      <td><strong>Use Cases</strong></td>
      <td>Vision, audio, multi-modal search, robotics</td>
      <td>Chatbots, Q&amp;A, summarization, agents, code-gen</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="section-2-training-basics">2. Training Hyperparameters You Must Know</h2>

<ul>
  <li><strong>Epochs</strong> ‚Äì One pass over the full training set. Multiple batches make up an epoch; models iterate through many epochs until accuracy converges or validation loss stops improving.</li>
  <li><strong>Batch size</strong> ‚Äì Number of records processed per update. Small batches fit on modest GPUs and introduce more gradient noise (helpful for generalization); large batches use more memory but stabilize training.</li>
  <li><strong>Learning rate</strong> ‚Äì Size of each weight update. High rates (e.g., <code class="language-plaintext highlighter-rouge">1e-1</code>) converge quickly but can overshoot; low rates (e.g., <code class="language-plaintext highlighter-rouge">5e-5</code> for BERT) are slower but safer. Most schedulers decay the rate as training progresses.</li>
</ul>

<p>Remember: adjusting these three knobs is often enough to fix unstable training before you consider changing the model architecture.</p>

<hr />

<h2 id="section-3-transformers">3. Why Transformers Matter</h2>

<p>Transformers replaced recurrent networks by relying on <strong>self-attention</strong>, which lets every token weigh every other token in the sequence to build contextual embeddings. Positional encodings preserve word order, and encoder/decoder stacks process inputs in parallel so latency scales well on GPUs. This architecture powers modern translation, summarization, speech, and vision-language models‚Äîand is the backbone of Bedrock and SageMaker JumpStart offerings.</p>

<hr />

<h2 id="section-4-generation-controls">4. Steering LLM Output</h2>

<ul>
  <li><strong>Temperature (0‚Äì1)</strong> ‚Äì Scales the probability distribution before sampling. Lower values push the model toward the single most likely answer (deterministic); higher values encourage creative or varied text.</li>
  <li><strong>Top-p (nucleus sampling)</strong> ‚Äì Keeps only the smallest set of tokens whose cumulative probability is ‚â• <code class="language-plaintext highlighter-rouge">p</code>. Lower <code class="language-plaintext highlighter-rouge">p</code> limits the candidate pool to highly probable tokens; higher <code class="language-plaintext highlighter-rouge">p</code> allows adventurous replies. Amazon Bedrock exposes both parameters for every supported model.</li>
</ul>

<hr />

<h2 id="section-5-rag">5. Retrieval-Augmented Generation (RAG)</h2>

<p>RAG keeps foundation models up to date without re-training:</p>

<ol>
  <li>Ingest documents (PDFs, FAQs, catalogs) and chunk them with metadata.</li>
  <li>Create embeddings and store them in a vector index such as OpenSearch Serverless, Aurora Postgres + pgvector, or Knowledge Bases for Amazon Bedrock.</li>
  <li>When the user asks a question, retrieve the most relevant chunks.</li>
  <li>Pass the question + retrieved context to the LLM so the answer cites fresh data.</li>
</ol>

<p>This pattern is cheaper than fine-tuning every time the knowledge base changes and is the default recommendation on the exam for ‚Äúmost current answers at low cost.‚Äù</p>

<hr />

<h2 id="section-6-specialization">6. Specializing a Foundation Model</h2>

<h3 id="section-6-1-domain-adaptation">6.1 Domain Adaptation Fine-Tuning</h3>

<ul>
  <li>Start with a general foundation model, then fine-tune on labeled domain-specific prompts/responses.</li>
  <li>Best when you have curated task data (support tickets, contracts, medical summaries) and need the model to follow strict formats or voice.</li>
  <li>Requires fewer tokens than pre-training because you are only nudging existing weights.</li>
</ul>

<h3 id="section-6-2-continued-pretraining">6.2 Continued Pre-Training</h3>

<ul>
  <li>Feed the model a large unlabeled corpus from your domain to extend its vocabulary and context understanding before supervised fine-tuning.</li>
  <li>Use this when jargon-heavy data (legal, biotech, oil &amp; gas) is missing from the public corpus. You still need to run a fine-tune afterward for task instructions.</li>
</ul>

<p>Exam tip: choose fine-tuning for adapting behavior on known tasks; choose continued pre-training when the base knowledge is insufficient.</p>

<hr />

<h2 id="section-7-embeddings">7. Embeddings and BERT</h2>

<p>Embedding models convert words, sentences, or images into dense vectors so that similar items land close together in multi-dimensional space. BERT (Bidirectional Encoder Representations from Transformers) generates contextual embeddings by looking at words both before and after the target token. Because embeddings change with context, BERT excels at intent detection, entity recognition, and semantic search where static word vectors fail.</p>

<hr />

<h2 id="section-8-aws-services">8. AWS AI Services Cheat Sheet</h2>

<table class="study-table">
  <thead>
    <tr>
      <th>Service</th>
      <th>Category</th>
      <th>What to Remember</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Amazon Bedrock</strong></td>
      <td>Foundation Models</td>
      <td>Serverless access to Titan, Anthropic, Meta, Cohere, Mistral models; built-in RAG and guardrails.</td>
    </tr>
    <tr>
      <td><strong>Amazon SageMaker</strong></td>
      <td>Build/Train/Deploy</td>
      <td>Custom training, fine-tuning, autopilot, and JumpStart model zoo.</td>
    </tr>
    <tr>
      <td><strong>Amazon Transcribe</strong></td>
      <td>Speech-to-Text</td>
      <td>Streaming/batch transcription with channel identification and redaction.</td>
    </tr>
    <tr>
      <td><strong>Amazon Comprehend</strong></td>
      <td>Natural Language</td>
      <td>Entity detection, sentiment, key phrases, PII redaction; supports custom classification.</td>
    </tr>
    <tr>
      <td><strong>Amazon Rekognition</strong></td>
      <td>Vision</td>
      <td>Image/video labels, face search, unsafe content, text detection.</td>
    </tr>
    <tr>
      <td><strong>Amazon Textract</strong></td>
      <td>Document AI</td>
      <td>Structured extraction (forms, tables) beyond OCR.</td>
    </tr>
    <tr>
      <td><strong>Amazon Polly</strong></td>
      <td>Text-to-Speech</td>
      <td>Neural voices, Speech Marks for lip-sync, Lex integration.</td>
    </tr>
    <tr>
      <td><strong>Amazon Lex</strong></td>
      <td>Conversational</td>
      <td>Build chat/voice bots with slots, Lambda fulfillment, multi-lingual support.</td>
    </tr>
    <tr>
      <td><strong>Amazon Translate</strong></td>
      <td>Machine Translation</td>
      <td>Real-time and batch translation with active custom terminology.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="section-9-llm-diagram">9. LLM Conversation Flow Diagram</h2>

<p>Use this UML snippet to visualize how user prompts move through retrieval, inference, and back to the UI. Add the PNG at <code class="language-plaintext highlighter-rouge">./assets/llm_conversation_flow.png</code> once you export it from PlantUML.</p>

<div class="image-wrapper">
  <img src="./assets/llm_conversation_flow.png" alt="llm_conversation_flow" class="modal-trigger" data-caption="üß† LLM Conversation Flow" />
  <div class="diagram-caption" data-snippet-id="llm_conversation_flow">
    üß† LLM Conversation Flow
  </div>
  <script type="text/plain" id="llm_conversation_flow">
@startuml
title LLM + RAG request lifecycle

actor User
participant "Application UI" as UI
participant "Retriever\n(Vector DB)" as RETRIEVE
participant "LLM Gateway\n(Bedrock/SageMaker)" as LLM
participant "Post-Processing" as POST

User -> UI: Ask question
UI -> RETRIEVE: search embeddings (top-k)
RETRIEVE --> UI: relevant context
UI -> LLM: prompt + context + controls\n(temp, top-p, max tokens)
LLM --> UI: streamed tokens
UI -> POST: apply guardrails\n(citations, formatting)
POST --> User: final answer + sources

@enduml
  </script>
</div>

<hr />

<h2 id="section-10-exam-reminders">10. Exam Reminders</h2>

<ul>
  <li>Always justify RAG when the requirement is ‚Äúfresh data without retraining.‚Äù</li>
  <li>Pick domain adaptation fine-tuning when the task format is known; pick continued pre-training when the vocabulary is missing.</li>
  <li>Control hallucinations by lowering temperature/top-p and grounding answers with retrieved context.</li>
  <li>Know the basic capability of each managed AI service‚Äîexam questions often ask you to swap Rekognition (vision) vs Textract (document parsing) vs Comprehend (text NLP).</li>
</ul>

<hr />

<h2 id="section-11-prompt-engineering">11. Prompt Engineering Essentials</h2>

<ul>
  <li><strong>Zero-shot vs few-shot</strong> ‚Äì Zero-shot relies purely on instructions, ideal for broad Q&amp;A; few-shot includes curated examples so the model mirrors tone or schema. Prefer few-shot when responses must follow strict formatting.</li>
  <li><strong>System vs user prompts</strong> ‚Äì System prompts set persona/policies; user prompts keep transient context. Adjust system prompts for compliance tone without retraining.</li>
  <li><strong>Structured outputs</strong> ‚Äì Describe the schema (JSON/YAML) and list mandatory/optional fields; combine with Bedrock Guardrails or Lex slot validation to reject malformed payloads.</li>
  <li><strong>Decoding controls</strong> ‚Äì Temperature, top-p, max tokens, and stop sequences govern creativity, latency, and runaway responses. Exams often ask you to lower temperature and add stop sequences to cap emails or policies.</li>
  <li><strong>Tool/function calling</strong> ‚Äì Document available functions (name + parameters). The LLM decides whether to call a tool and returns JSON so the orchestrator can act. Key concept for Bedrock Agents and custom controllers.</li>
</ul>

<p><em>Exam tip: Keep prompts short, move reusable policy text to templates, and cap <code class="language-plaintext highlighter-rouge">max_tokens</code> to avoid surprise billing.</em></p>

<hr />

<h2 id="section-12-rag-architecture">12. RAG Architecture &amp; Tuning</h2>

<ul>
  <li><strong>Chunking + overlap</strong> ‚Äì Target 200‚Äì500 token chunks with 10‚Äì20% overlap so passages maintain context without blowing up storage.</li>
  <li><strong>Embeddings + vector stores</strong> ‚Äì Use Titan Text Embeddings, Cohere Embed, or open-source (bge/e5) with OpenSearch Serverless, Aurora pgvector, Neptune Analytics, or Bedrock Knowledge Bases.</li>
  <li><strong>Top-k retrieval + metadata filters</strong> ‚Äì Start with <code class="language-plaintext highlighter-rouge">k=3‚Äì5</code>, then filter by product, locale, or classification labels to cut noise.</li>
  <li><strong>Reranking + hybrid search</strong> ‚Äì Combine dense vectors with keyword/BM25 or use rerankers when rare terms or legal phrases matter.</li>
  <li><strong>Latency vs cost</strong> ‚Äì Larger vectors improve recall but add milliseconds and storage fees. Cache popular context windows; pre-compute embeddings offline.</li>
</ul>

<div class="image-wrapper">
  <img src="./assets/rag_architecture.png" alt="rag_architecture" class="modal-trigger" data-caption="üß± RAG Architecture" />
  <div class="diagram-caption" data-snippet-id="rag_architecture">
    üß± RAG Architecture
  </div>
  <script type="text/plain" id="rag_architecture">
@startuml
title End-to-end RAG flow

actor User
participant "Client App" as APP
participant "Chunk & Embed" as INGEST
participant "Vector Store" as VECTOR
participant "LLM Endpoint" as LLM

== Ingestion ==
APP -> INGEST: Upload docs + metadata
INGEST -> VECTOR: Store embeddings + filters

== Query ==
User -> APP: Question
APP -> VECTOR: similarity search (top-k + filter)
VECTOR --> APP: relevant chunks
APP -> LLM: prompt + chunks + decoding controls
LLM --> APP: grounded answer + citations
APP --> User: response (low hallucination)

@enduml
  </script>
</div>

<p><em>Exam tip: Answer ‚ÄúUse Bedrock Knowledge Bases‚Äù whenever the question stresses managed ingestion + retrieval.</em></p>

<hr />

<h2 id="section-13-agents">13. Agents &amp; Tool Use</h2>

<ul>
  <li><strong>Agent vs RAG</strong> ‚Äì Use RAG for better context inside a single response. Use agents when you need planning, multi-step workflows, or to call APIs/DBs dynamically.</li>
  <li><strong>Tool calling patterns</strong> ‚Äì The agent interprets intent, chooses a tool (Lambda/HTTPS/Step Functions), executes it, ingests the result, and may loop until complete.</li>
  <li><strong>AWS mapping</strong> ‚Äì Bedrock Agents orchestrate reasoning, call Lambda for business logic, Step Functions for long-running jobs, and can integrate RAG for grounding.</li>
</ul>

<div class="image-wrapper">
  <img src="./assets/agent_tool_chain.png" alt="agent_tool_chain" class="modal-trigger" data-caption="ü§ñ Agent Tool Chain" />
  <div class="diagram-caption" data-snippet-id="agent_tool_chain">
    ü§ñ Agent Tool Chain
  </div>
  <script type="text/plain" id="agent_tool_chain">
@startuml
title Bedrock Agent invoking tools

actor User
participant "Bedrock Agent" as AGENT
participant "RAG Retriever" as RAG
participant "Lambda Tool" as LAMBDA
participant "Step Functions" as STEP
participant "Final Response" as RESP

User -> AGENT: Complex task
AGENT -> RAG: fetch context (optional)
RAG --> AGENT: supporting data
AGENT -> LAMBDA: call tool (JSON params)
LAMBDA --> AGENT: tool output
AGENT -> STEP: orchestrate long workflow
STEP --> AGENT: completion state
AGENT -> RESP: compose answer + actions
RESP --> User: explanation + citations

@enduml
  </script>
</div>

<p><em>Exam tip: If the requirement says ‚Äúcall internal APIs and external SaaS with reasoning,‚Äù pick Bedrock Agents with Lambda tools.</em></p>

<hr />

<h2 id="section-14-eval">14. Evaluation &amp; Hallucination Control</h2>

<ul>
  <li><strong>Offline evaluation</strong> ‚Äì Score prompts/models against golden datasets for accuracy, BLEU/ROUGE, or custom rubric. Automate in SageMaker pipelines.</li>
  <li><strong>Human + LLM-as-judge</strong> ‚Äì SMEs validate edge cases; LLM judges accelerate regression testing but must be calibrated.</li>
  <li><strong>Grounding + citations</strong> ‚Äì Return snippet IDs or URLs so auditors can verify answers. Bedrock Knowledge Bases can include citations automatically.</li>
  <li><strong>Temperature + guardrails</strong> ‚Äì Keep temperature/top-p low for factual tasks and add Guardrails for profanity/PII filters or JSON schemas.</li>
</ul>

<p><em>Exam tip: When compliance reviewers are mentioned, respond with ‚Äúhuman-in-the-loop evaluation plus logged citations.‚Äù</em></p>

<hr />

<h2 id="section-15-responsible-ai">15. Responsible AI, Privacy, Security</h2>

<ul>
  <li><strong>Bias/fairness</strong> ‚Äì Audit datasets, compare outputs across demographic slices, and document mitigations.</li>
  <li><strong>PII governance</strong> ‚Äì Mask prompts, encrypt data (KMS), route calls through VPC endpoints/PrivateLink, and restrict IAM roles for agents/tools.</li>
  <li><strong>Hallucination + safety</strong> ‚Äì Pair RAG grounding with Guardrails to block unsafe content and require human review for high-risk actions.</li>
  <li><strong>AWS mapping</strong> ‚Äì Use Bedrock Guardrails, IAM least privilege, CloudTrail logging, and encrypted vector stores/S3 buckets.</li>
</ul>

<hr />

<h2 id="section-16-cost">16. Cost Optimization for GenAI</h2>

<ul>
  <li><strong>Token drivers</strong> ‚Äì Shorten system prompts, trim few-shot examples, and cap max tokens. Every unused token is direct cost.</li>
  <li><strong>Caching</strong> ‚Äì Cache embeddings, retrieval hits, and deterministic responses to avoid re-querying the LLM.</li>
  <li><strong>Model sizing</strong> ‚Äì Start with smaller models (e.g., 13B) and scale up only if KPIs demand it. Use distillation or parameter-efficient fine-tuning for narrow tasks.</li>
  <li><strong>Batch vs real-time</strong> ‚Äì Batch summarize archives or compliance logs; reserve real-time endpoints for interactive needs. SageMaker and Bedrock both support asynchronous invocations.</li>
  <li><strong>RAG vs fine-tune vs prompt</strong> ‚Äì RAG minimizes retraining when knowledge updates frequently, fine-tuning lowers per-request tokens for repetitive tasks, and improved prompting can defer expensive training entirely.</li>
</ul>

<p><em>Exam tip: Mention ‚Äúuse Bedrock serverless invocation + caching‚Äù whenever the question references ‚Äúspiky demand‚Äù or ‚Äúcost control.‚Äù</em></p>

<hr />

<h2 id="section-17-decision-matrix">17. Decision Matrix</h2>

<table class="study-table">
  <thead>
    <tr>
      <th>Approach</th>
      <th>When to Choose</th>
      <th>Exam Trigger Words</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Prompting</strong></td>
      <td>General knowledge, low customization, rapid experimentation.</td>
      <td>‚ÄúNo training budget,‚Äù ‚Äúquick prototype.‚Äù</td>
    </tr>
    <tr>
      <td><strong>RAG</strong></td>
      <td>Fresh proprietary data, citations, large document sets.</td>
      <td>‚ÄúLatest manuals,‚Äù ‚Äúground answers,‚Äù ‚Äúno retraining.‚Äù</td>
    </tr>
    <tr>
      <td><strong>Fine-tuning</strong></td>
      <td>Strict formats, domain tone, curated labeled data.</td>
      <td>‚ÄúConsistent summaries,‚Äù ‚Äúapproved style guide.‚Äù</td>
    </tr>
    <tr>
      <td><strong>Continued Pretraining</strong></td>
      <td>Missing vocabulary/jargon, massive unlabeled domain corpus.</td>
      <td>‚ÄúIndustry-specific terms,‚Äù ‚Äúexpand base knowledge.‚Äù</td>
    </tr>
    <tr>
      <td><strong>Agents</strong></td>
      <td>Multi-step reasoning, tool invocation, integrations.</td>
      <td>‚ÄúPlan workflow,‚Äù ‚Äúcall APIs,‚Äù ‚Äútake actions.‚Äù</td>
    </tr>
  </tbody>
</table>

<hr />

    </article>
  </main>

  <div id="imageModal" class="image-modal">
  <div class="modal-content">
    <span class="modal-close">&times;</span>
    <button class="modal-nav modal-prev">&#8249;</button>
    <button class="modal-nav modal-next">&#8250;</button>
    <img id="modalImage" src="" alt="">
    <div id="modalCaption"></div>
  </div>
</div>

<script>
document.addEventListener('DOMContentLoaded', () => {
  const modal = document.getElementById('imageModal');
  const modalImg = document.getElementById('modalImage');
  const modalCaption = document.getElementById('modalCaption');
  const modalClose = document.querySelector('.modal-close');
  const modalPrev = document.querySelector('.modal-prev');
  const modalNext = document.querySelector('.modal-next');

  const SCALE_STEP = 0.2;
  const MIN_SCALE = 1;
  const MAX_SCALE = 3;

  let modalImages = [];
  let currentIndex = -1;
  let currentScale = MIN_SCALE;
  let translateX = 0;
  let translateY = 0;
  let isDragging = false;
  let isTouchDragging = false;
  let isPinching = false;
  let dragStartX = 0;
  let dragStartY = 0;
  let pinchStartDistance = 0;
  let pinchStartScale = MIN_SCALE;
  let transitionTimeout;

  function refreshModalImages() {
    modalImages = Array.from(document.querySelectorAll('.modal-trigger'));
  }

  function applyTransform(withTransition = false) {
    if (currentScale <= MIN_SCALE) {
      currentScale = MIN_SCALE;
      translateX = 0;
      translateY = 0;
    }

    if (transitionTimeout) {
      clearTimeout(transitionTimeout);
      transitionTimeout = null;
    }

    modalImg.style.transition = withTransition ? 'transform 0.15s ease' : 'none';
    modalImg.style.transform = `translate(${translateX}px, ${translateY}px) scale(${currentScale})`;
    modalImg.style.cursor = currentScale > MIN_SCALE
      ? (isDragging || isTouchDragging ? 'grabbing' : 'grab')
      : 'auto';

    if (withTransition) {
      transitionTimeout = setTimeout(() => {
        modalImg.style.transition = 'none';
      }, 160);
    }
  }

  function resetTransform() {
    currentScale = MIN_SCALE;
    translateX = 0;
    translateY = 0;
    applyTransform(false);
  }

  function getDistance(touch1, touch2) {
    return Math.hypot(
      touch1.clientX - touch2.clientX,
      touch1.clientY - touch2.clientY
    );
  }

  function openModalAtIndex(index) {
    refreshModalImages();

    if (!modalImages.length) {
      return;
    }

    if (index < 0) {
      index = modalImages.length - 1;
    } else if (index >= modalImages.length) {
      index = 0;
    }

    const target = modalImages[index];
    if (!target) {
      return;
    }

    currentIndex = index;
    const src = target.dataset.modalSrc || target.src;
    const caption = target.getAttribute('data-caption') || target.alt;

    modalImg.src = src;
    modalCaption.textContent = caption || '';
    modal.classList.add('show');
    document.body.style.overflow = 'hidden';
    resetTransform();
  }

  function closeModal() {
    if (!modal.classList.contains('show')) {
      return;
    }
    modal.classList.remove('show');
    document.body.style.overflow = '';
    currentIndex = -1;
    isDragging = false;
    isTouchDragging = false;
    isPinching = false;
    resetTransform();
  }

  refreshModalImages();

  modalImg.addEventListener('dragstart', event => event.preventDefault());

  document.addEventListener('click', event => {
    const trigger = event.target.closest('.modal-trigger');
    if (!trigger) {
      return;
    }

    event.preventDefault();
    refreshModalImages();
    const index = modalImages.indexOf(trigger);
    openModalAtIndex(index > -1 ? index : 0);
  });

  modalClose?.addEventListener('click', closeModal);

  modal.addEventListener('click', event => {
    if (event.target === modal) {
      closeModal();
    }
  });

  document.addEventListener('keydown', event => {
    if (!modal.classList.contains('show')) {
      return;
    }

    switch (event.key) {
      case 'Escape':
        closeModal();
        break;
      case 'ArrowLeft':
        event.preventDefault();
        openModalAtIndex(currentIndex - 1);
        break;
      case 'ArrowRight':
        event.preventDefault();
        openModalAtIndex(currentIndex + 1);
        break;
      case '+':
      case '=':
        event.preventDefault();
        currentScale = Math.min(MAX_SCALE, currentScale + SCALE_STEP);
        applyTransform(true);
        break;
      case '-':
        event.preventDefault();
        currentScale = Math.max(MIN_SCALE, currentScale - SCALE_STEP);
        applyTransform(true);
        break;
      case '0':
        event.preventDefault();
        resetTransform();
        applyTransform(true);
        break;
      default:
        break;
    }
  });

  modal.addEventListener('wheel', event => {
    if (!modal.classList.contains('show')) {
      return;
    }

    event.preventDefault();
    const delta = event.deltaY > 0 ? -SCALE_STEP : SCALE_STEP;
    const nextScale = Math.max(MIN_SCALE, Math.min(MAX_SCALE, currentScale + delta));

    if (nextScale === currentScale) {
      return;
    }

    currentScale = nextScale;
    applyTransform(true);
  }, { passive: false });

  if (modalPrev) {
    modalPrev.addEventListener('click', event => {
      event.preventDefault();
      if (currentIndex === -1) {
        return;
      }
      openModalAtIndex(currentIndex - 1);
    });
  }

  if (modalNext) {
    modalNext.addEventListener('click', event => {
      event.preventDefault();
      if (currentIndex === -1) {
        return;
      }
      openModalAtIndex(currentIndex + 1);
    });
  }

  modalImg.addEventListener('mousedown', event => {
    if (!modal.classList.contains('show') || currentScale <= MIN_SCALE) {
      return;
    }

    event.preventDefault();
    isDragging = true;
    dragStartX = event.clientX;
    dragStartY = event.clientY;
    modalImg.style.cursor = 'grabbing';
  });

  document.addEventListener('mousemove', event => {
    if (!isDragging) {
      return;
    }

    event.preventDefault();
    translateX += event.clientX - dragStartX;
    translateY += event.clientY - dragStartY;
    dragStartX = event.clientX;
    dragStartY = event.clientY;
    applyTransform(false);
  });

  document.addEventListener('mouseup', () => {
    if (!isDragging) {
      return;
    }
    isDragging = false;
    modalImg.style.cursor = currentScale > MIN_SCALE ? 'grab' : 'auto';
  });

  modalImg.addEventListener('touchstart', event => {
    if (!modal.classList.contains('show')) {
      return;
    }

    if (event.touches.length === 1 && currentScale > MIN_SCALE) {
      event.preventDefault();
      isTouchDragging = true;
      dragStartX = event.touches[0].clientX;
      dragStartY = event.touches[0].clientY;
      modalImg.style.cursor = 'grabbing';
    } else if (event.touches.length === 2) {
      event.preventDefault();
      isTouchDragging = false;
      isPinching = true;
      pinchStartDistance = getDistance(event.touches[0], event.touches[1]);
      pinchStartScale = currentScale;
      modalImg.style.cursor = currentScale > MIN_SCALE ? 'grab' : 'auto';
    }
  }, { passive: false });

  modalImg.addEventListener('touchmove', event => {
    if (!modal.classList.contains('show')) {
      return;
    }

    if (isPinching && event.touches.length === 2) {
      event.preventDefault();
      const distance = getDistance(event.touches[0], event.touches[1]);
      if (pinchStartDistance > 0) {
        const scaleRatio = distance / pinchStartDistance;
        currentScale = Math.max(MIN_SCALE, Math.min(MAX_SCALE, pinchStartScale * scaleRatio));
        applyTransform(false);
      }
    } else if (isTouchDragging && event.touches.length === 1) {
      event.preventDefault();
      translateX += event.touches[0].clientX - dragStartX;
      translateY += event.touches[0].clientY - dragStartY;
      dragStartX = event.touches[0].clientX;
      dragStartY = event.touches[0].clientY;
      applyTransform(false);
    }
  }, { passive: false });

  modalImg.addEventListener('touchend', event => {
    if (!modal.classList.contains('show')) {
      return;
    }

    if (event.touches.length === 0) {
      isTouchDragging = false;
      modalImg.style.cursor = currentScale > MIN_SCALE ? 'grab' : 'auto';
    }

    if (event.touches.length < 2) {
      isPinching = false;
      pinchStartDistance = 0;
      pinchStartScale = currentScale;
    }
  });

  modalImg.addEventListener('touchcancel', () => {
    if (!modal.classList.contains('show')) {
      return;
    }

    isTouchDragging = false;
    isPinching = false;
    pinchStartDistance = 0;
    pinchStartScale = currentScale;
    modalImg.style.cursor = currentScale > MIN_SCALE ? 'grab' : 'auto';
  });

  modal.addEventListener('touchend', event => {
    if (!modal.classList.contains('show')) {
      return;
    }

    if (event.target === modal && event.changedTouches.length === 1 && !isTouchDragging && !isPinching) {
      closeModal();
    }
  }, { passive: true });
});
</script>

<script>
document.addEventListener('DOMContentLoaded', () => {
  document.querySelectorAll('[data-snippet-id]').forEach(caption => {
    const snippetId = caption.getAttribute('data-snippet-id');
    const snippetElement = document.getElementById(snippetId);

    if (snippetElement) {
      const content = snippetElement.textContent.replace(/"/g, '&quot;');
      caption.setAttribute('data-tooltip', content);
      caption.classList.add('diagram-caption');
    }
  });
});
</script>


  <!-- Floating Table of Contents -->
  <div id="floating-toc" class="floating-toc">
    <div class="toc-icon">
      <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <line x1="8" y1="6" x2="21" y2="6"></line>
        <line x1="8" y1="12" x2="21" y2="12"></line>
        <line x1="8" y1="18" x2="21" y2="18"></line>
        <line x1="3" y1="6" x2="3.01" y2="6"></line>
        <line x1="3" y1="12" x2="3.01" y2="12"></line>
        <line x1="3" y1="18" x2="3.01" y2="18"></line>
      </svg>
    </div>
    <div class="toc-content">
      <div class="toc-header">
        <span>Contents</span>
        <button class="toc-close" aria-label="Close table of contents">
          <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <line x1="18" y1="6" x2="6" y2="18"></line>
            <line x1="6" y1="6" x2="18" y2="18"></line>
          </svg>
        </button>
      </div>
      <nav class="toc-nav" id="toc-nav">
        <!-- Generated by JavaScript -->
      </nav>
    </div>
  </div>

  <footer class="site-footer">
    <span>¬© 2026 glucolte ‚Ä¢ </span>
    <a href="https://github.com/gLuColte/glucolte.github.io">source</a>
  </footer>

  <script>
    // Dark mode functionality
    function initDarkMode() {
      const toggle = document.getElementById('dark-mode-toggle');
      const body = document.body;
      
      // Check for saved theme preference or default to light mode
      const savedTheme = localStorage.getItem('theme');
      const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
      
      if (savedTheme === 'dark' || (!savedTheme && prefersDark)) {
        body.setAttribute('data-theme', 'dark');
      }
      
      // Toggle dark mode
      if (toggle) {
        toggle.addEventListener('click', function() {
          const currentTheme = body.getAttribute('data-theme');
          const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
          
          body.setAttribute('data-theme', newTheme);
          localStorage.setItem('theme', newTheme);
        });
      }
    }
    
    // Initialize dark mode immediately to prevent flash
    initDarkMode();
    
    // Initialize highlight.js and floating TOC after the DOM is loaded
    document.addEventListener('DOMContentLoaded', function() {
      hljs.highlightAll();
      
      // Floating TOC functionality - only show on sub-study pages
      const floatingToc = document.getElementById('floating-toc');
      const tocNav = document.getElementById('toc-nav');
      const tocClose = document.querySelector('.toc-close');
      
      // Check if we're on a sub-study page (not the main study index)
      const isSubStudyPage = window.location.pathname !== '/study/' && 
                            window.location.pathname !== '/study' && 
                            window.location.pathname.startsWith('/study/');
      
      if (floatingToc && tocNav && isSubStudyPage) {
        // Generate TOC from headings
        function generateTOC() {
          const headings = document.querySelectorAll('.content h1, .content h2, .content h3, .content h4, .content h5, .content h6');
          if (headings.length === 0) {
            floatingToc.style.display = 'none';
            return;
          }
          
          const tocList = document.createElement('ul');
          let currentLevel = 0;
          let currentList = tocList;
          const listStack = [tocList];
          
          headings.forEach((heading, index) => {
            const level = parseInt(heading.tagName.charAt(1));
            const id = heading.id || `heading-${index}`;
            
            // Ensure heading has an ID
            if (!heading.id) {
              heading.id = id;
            }
            
            // Create list item
            const listItem = document.createElement('li');
            const link = document.createElement('a');
            link.href = `#${id}`;
            link.textContent = heading.textContent.trim();
            link.addEventListener('click', function(e) {
              e.preventDefault();
              const targetElement = document.getElementById(id);
              if (targetElement) {
                targetElement.scrollIntoView({ behavior: 'smooth', block: 'start' });
                // Close TOC after clicking
                floatingToc.classList.remove('toc-open');
              }
            });
            
            listItem.appendChild(link);
            
            // Handle nesting
            if (level > currentLevel) {
              // Go deeper
              for (let i = currentLevel; i < level; i++) {
                const newList = document.createElement('ul');
                currentList.appendChild(newList);
                listStack.push(newList);
                currentList = newList;
              }
            } else if (level < currentLevel) {
              // Go shallower
              for (let i = currentLevel; i > level; i--) {
                listStack.pop();
                currentList = listStack[listStack.length - 1];
              }
            }
            
            currentList.appendChild(listItem);
            currentLevel = level;
          });
          
          tocNav.appendChild(tocList);
        }
        
        // Generate the TOC
        generateTOC();
        
        // Handle close button
        if (tocClose) {
          tocClose.addEventListener('click', function() {
            floatingToc.classList.remove('toc-open');
          });
        }
        
        // Handle click on icon to toggle
        const tocIcon = document.querySelector('.toc-icon');
        if (tocIcon) {
          tocIcon.addEventListener('click', function() {
            floatingToc.classList.toggle('toc-open');
          });
        }
        
        // Active section highlighting
        function updateActiveSection() {
          const headings = document.querySelectorAll('.content h1, .content h2, .content h3, .content h4, .content h5, .content h6');
          const tocLinks = tocNav.querySelectorAll('a');
          
          // Remove active class from all links
          tocLinks.forEach(link => link.classList.remove('active'));
          
          // Find the current section
          let currentHeading = null;
          const scrollPosition = window.scrollY + 100; // Offset for better UX
          
          for (let i = headings.length - 1; i >= 0; i--) {
            const heading = headings[i];
            const headingTop = heading.offsetTop;
            
            if (headingTop <= scrollPosition) {
              currentHeading = heading;
              break;
            }
          }
          
          // Add active class to current section
          if (currentHeading) {
            const activeLink = tocNav.querySelector(`a[href="#${currentHeading.id}"]`);
            if (activeLink) {
              activeLink.classList.add('active');
            }
          }
        }
        
        // Update active section on scroll
        let scrollTimeout;
        window.addEventListener('scroll', function() {
          if (scrollTimeout) {
            clearTimeout(scrollTimeout);
          }
          scrollTimeout = setTimeout(updateActiveSection, 10);
        });
        
        // Initial update
        updateActiveSection();
      } else if (floatingToc && !isSubStudyPage) {
        // Hide floating TOC on main study page
        floatingToc.style.display = 'none';
      }
      
      // Legacy TOC functionality (for existing markdown TOCs)
      const toc = document.querySelector('#markdown-toc');
      if (toc) {
        // Hide the floating TOC if there's already a markdown TOC
        if (floatingToc) {
          floatingToc.style.display = 'none';
        }
        
        // Create TOC container with header
        const tocContainer = document.createElement('div');
        tocContainer.className = 'toc-container';
        tocContainer.style.cssText = `
          background: #f8fafc;
          border: 1px solid #e5e7eb;
          border-radius: 8px;
          padding: 16px;
          margin: 20px 0;
          position: relative;
        `;
        
        // Add TOC header
        const tocHeader = document.createElement('h3');
        tocHeader.textContent = 'Table of Contents';
        tocHeader.style.cssText = `
          margin: 0 0 12px 0;
          font-size: 16px;
          font-weight: 600;
          color: #374151;
        `;
        
        // Add back to top button
        const backToTopBtn = document.createElement('button');
        backToTopBtn.textContent = '‚Üë Top';
        backToTopBtn.style.cssText = `
          position: absolute;
          top: 12px;
          right: 12px;
          background: #ffffff;
          border: 1px solid #d1d5db;
          border-radius: 6px;
          padding: 4px 8px;
          font-size: 12px;
          cursor: pointer;
          color: #6b7280;
          transition: all 0.2s ease;
        `;
        
        // Add hover effect for back to top button
        backToTopBtn.addEventListener('mouseenter', function() {
          this.style.background = '#f3f4f6';
          this.style.color = '#374151';
        });
        
        backToTopBtn.addEventListener('mouseleave', function() {
          this.style.background = '#ffffff';
          this.style.color = '#6b7280';
        });
        
        // Back to top functionality
        backToTopBtn.addEventListener('click', function() {
          window.scrollTo({ top: 0, behavior: 'smooth' });
        });
        
        // Style the TOC list
        toc.style.cssText = `
          margin: 0;
          padding: 0;
          font-size: 14px;
          line-height: 1.5;
        `;
        
        // Insert header and button into container
        tocContainer.appendChild(tocHeader);
        tocContainer.appendChild(backToTopBtn);
        tocContainer.appendChild(toc);
        
        // Replace the original TOC with the new container
        toc.parentNode.insertBefore(tocContainer, toc);
        tocContainer.appendChild(toc);
        
        // Add smooth scrolling to TOC links
        const tocLinks = toc.querySelectorAll('a');
        tocLinks.forEach(link => {
          link.addEventListener('click', function(e) {
            e.preventDefault();
            const targetId = this.getAttribute('href').substring(1);
            const targetElement = document.getElementById(targetId);
            if (targetElement) {
              targetElement.scrollIntoView({ behavior: 'smooth' });
            }
          });
        });
      }
    });
  </script>
</body>
</html>
